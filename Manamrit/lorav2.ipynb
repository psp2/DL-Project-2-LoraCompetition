{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a02285e6",
   "metadata": {
    "id": "a02285e6"
   },
   "source": [
    "# Starter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcc5329",
   "metadata": {
    "id": "bdcc5329"
   },
   "source": [
    "Install and import required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d374fa7e",
   "metadata": {},
   "source": [
    "91.39% 83.75% smaller batch size, warmup, regularization, lower LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "348ceed6-b684-46c3-8a32-9bb640c9a9d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T03:15:41.690963Z",
     "iopub.status.busy": "2025-04-12T03:15:41.690720Z",
     "iopub.status.idle": "2025-04-12T03:15:48.470083Z",
     "shell.execute_reply": "2025-04-12T03:15:48.468901Z",
     "shell.execute_reply.started": "2025-04-12T03:15:41.690945Z"
    },
    "id": "348ceed6-b684-46c3-8a32-9bb640c9a9d7",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-ml-py3\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: nvidia-ml-py3\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19173 sha256=e0131f18875e3869c66e1cb750ab48793865ee1e1aa85c7e1cb7612917c6de85\n",
      "  Stored in directory: /root/.cache/pip/wheels/47/50/9e/29dc79037d74c3c1bb4a8661fb608e8674b7e4260d6a3f8f51\n",
      "Successfully built nvidia-ml-py3\n",
      "Installing collected packages: nvidia-ml-py3\n",
      "Successfully installed nvidia-ml-py3-7.352.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers datasets evaluate accelerate peft trl bitsandbytes\n",
    "!pip install nvidia-ml-py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cca64f38-d8d2-4313-8295-fbbd43c2a263",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T03:15:53.826544Z",
     "iopub.status.busy": "2025-04-12T03:15:53.825955Z",
     "iopub.status.idle": "2025-04-12T03:16:22.331885Z",
     "shell.execute_reply": "2025-04-12T03:16:22.331272Z",
     "shell.execute_reply.started": "2025-04-12T03:15:53.826516Z"
    },
    "id": "cca64f38-d8d2-4313-8295-fbbd43c2a263",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 03:16:08.279326: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744427768.501563      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744427768.565283      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaModel, RobertaTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding, RobertaForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from datasets import load_dataset, Dataset, ClassLabel\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6e377",
   "metadata": {
    "id": "59d6e377"
   },
   "source": [
    "## Load Tokenizer and Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467d0517-f433-4a94-9b89-ee66ba5fd95d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21f42747-f551-40a5-a95f-7affb1eba4a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T03:17:05.415850Z",
     "iopub.status.busy": "2025-04-12T03:17:05.415153Z",
     "iopub.status.idle": "2025-04-12T03:18:08.307219Z",
     "shell.execute_reply": "2025-04-12T03:18:08.306664Z",
     "shell.execute_reply.started": "2025-04-12T03:17:05.415827Z"
    },
    "id": "21f42747-f551-40a5-a95f-7affb1eba4a3",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4272576452849719f7d75b6778e0e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0fd3bf9dc2488b9ad7bc7f0823c5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a812481f194fc6bffaeeef6a76105b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7504ee77a06247eba8892e64578605fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba24aec404464eaaa949e891fc8abd04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977e27449ed143d1aa33c2be0d91a443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd74c8464a754a30957f20e18115e36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904684b4a4fd4227a1f2e5eda40a8c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e834872d32d4519af9dc318192c7e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df13f20114954b1081f0021aef9306e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec85968358e748a9aa5b84ba251e8aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = 'roberta-base'\n",
    "\n",
    "dataset = load_dataset('ag_news', split='train')\n",
    "tokenizer = RobertaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "def preprocess(examples):\n",
    "    tokenized = tokenizer(examples['text'], truncation=True, padding=True)\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True,  remove_columns=[\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e07f641-bec0-43a6-8c26-510d7642916a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T03:18:15.195725Z",
     "iopub.status.busy": "2025-04-12T03:18:15.194946Z",
     "iopub.status.idle": "2025-04-12T03:18:15.201131Z",
     "shell.execute_reply": "2025-04-12T03:18:15.200449Z",
     "shell.execute_reply.started": "2025-04-12T03:18:15.195699Z"
    },
    "id": "9e07f641-bec0-43a6-8c26-510d7642916a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of labels: 4\n",
      "the labels: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    }
   ],
   "source": [
    "# Extract the number of classess and their names\n",
    "num_labels = dataset.features['label'].num_classes\n",
    "class_names = dataset.features[\"label\"].names\n",
    "print(f\"number of labels: {num_labels}\")\n",
    "print(f\"the labels: {class_names}\")\n",
    "\n",
    "# Create an id2label mapping\n",
    "# We will need this for our classifier.\n",
    "id2label = {i: label for i, label in enumerate(class_names)}\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e24afd",
   "metadata": {
    "id": "c9e24afd"
   },
   "source": [
    "## Load Pre-trained Model\n",
    "Set up config for pretrained model and download it from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "262a8416-a59c-4ea1-95d9-0b1f81d6094c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T03:18:19.342178Z",
     "iopub.status.busy": "2025-04-12T03:18:19.341883Z",
     "iopub.status.idle": "2025-04-12T03:18:21.895559Z",
     "shell.execute_reply": "2025-04-12T03:18:21.894805Z",
     "shell.execute_reply.started": "2025-04-12T03:18:19.342156Z"
    },
    "id": "262a8416-a59c-4ea1-95d9-0b1f81d6094c",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779d07f04395427380f8ef8a2b500090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    base_model,\n",
    "    id2label=id2label)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f265839d-a088-4693-8474-862641de11ed",
   "metadata": {
    "id": "f265839d-a088-4693-8474-862641de11ed"
   },
   "source": [
    "## Anything from here on can be modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7413430-be57-482b-856e-36bd4ba799df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T03:18:26.737258Z",
     "iopub.status.busy": "2025-04-12T03:18:26.736540Z",
     "iopub.status.idle": "2025-04-12T03:18:26.775833Z",
     "shell.execute_reply": "2025-04-12T03:18:26.775053Z",
     "shell.execute_reply.started": "2025-04-12T03:18:26.737219Z"
    },
    "id": "e7413430-be57-482b-856e-36bd4ba799df",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110000, 3)\n",
      "(10000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Split the original training set\n",
    "split_datasets = tokenized_dataset.train_test_split(test_size=10000, seed=42)\n",
    "train_dataset = split_datasets['train']\n",
    "eval_dataset = split_datasets['test']\n",
    "\n",
    "print(train_dataset.shape)\n",
    "print(eval_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652452e3",
   "metadata": {
    "id": "652452e3"
   },
   "source": [
    "## Setup LoRA Config\n",
    "Setup PEFT config and get peft model for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0536cda-3139-4b22-9cd8-5213be78fc96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T03:18:32.070175Z",
     "iopub.status.busy": "2025-04-12T03:18:32.069412Z",
     "iopub.status.idle": "2025-04-12T03:19:51.245423Z",
     "shell.execute_reply": "2025-04-12T03:19:51.244418Z",
     "shell.execute_reply.started": "2025-04-12T03:18:32.070148Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.5.1+cu124)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.30.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.13.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2024.12.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "!pip install peft accelerate transformers datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd0ca0ea-86b8-47f7-8cbf-83da25685876",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T03:20:45.700590Z",
     "iopub.status.busy": "2025-04-12T03:20:45.699961Z",
     "iopub.status.idle": "2025-04-12T03:20:45.760938Z",
     "shell.execute_reply": "2025-04-12T03:20:45.760358Z",
     "shell.execute_reply.started": "2025-04-12T03:20:45.700559Z"
    },
    "id": "bd0ca0ea-86b8-47f7-8cbf-83da25685876",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): RobertaForSequenceClassification(\n",
       "      (roberta): RobertaModel(\n",
       "        (embeddings): RobertaEmbeddings(\n",
       "          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): RobertaEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSdpaSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): RobertaClassificationHead(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): RobertaClassificationHead(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # rank\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"value\"],  \n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS  \n",
    ")\n",
    "\n",
    "# Wrap the model with PEFT\n",
    "model = get_peft_model(model, lora_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ec2739d-76b6-4fde-91c2-0fc49e1884b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T03:20:51.446297Z",
     "iopub.status.busy": "2025-04-12T03:20:51.445490Z",
     "iopub.status.idle": "2025-04-12T03:20:51.483537Z",
     "shell.execute_reply": "2025-04-12T03:20:51.482827Z",
     "shell.execute_reply.started": "2025-04-12T03:20:51.446264Z"
    },
    "id": "6ec2739d-76b6-4fde-91c2-0fc49e1884b0",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/mapping.py:185: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'roberta-base' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): PeftModelForSequenceClassification(\n",
       "      (base_model): LoraModel(\n",
       "        (model): RobertaForSequenceClassification(\n",
       "          (roberta): RobertaModel(\n",
       "            (embeddings): RobertaEmbeddings(\n",
       "              (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "              (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "              (token_type_embeddings): Embedding(1, 768)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (encoder): RobertaEncoder(\n",
       "              (layer): ModuleList(\n",
       "                (0-11): 12 x RobertaLayer(\n",
       "                  (attention): RobertaAttention(\n",
       "                    (self): RobertaSdpaSelfAttention(\n",
       "                      (query): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (output): RobertaSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (intermediate): RobertaIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): RobertaOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (classifier): ModulesToSaveWrapper(\n",
       "            (original_module): RobertaClassificationHead(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "            )\n",
       "            (modules_to_save): ModuleDict(\n",
       "              (default): RobertaClassificationHead(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a769f54e-05ad-4e3c-aae8-d00d1d9dfb2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T03:21:03.570524Z",
     "iopub.status.busy": "2025-04-12T03:21:03.570195Z",
     "iopub.status.idle": "2025-04-12T03:21:03.577079Z",
     "shell.execute_reply": "2025-04-12T03:21:03.576291Z",
     "shell.execute_reply.started": "2025-04-12T03:21:03.570504Z"
    },
    "id": "a769f54e-05ad-4e3c-aae8-d00d1d9dfb2f",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "base_model.model.base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight\n",
      "base_model.model.base_model.model.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight\n",
      "base_model.model.base_model.model.classifier.modules_to_save.default.dense.weight\n",
      "base_model.model.base_model.model.classifier.modules_to_save.default.dense.bias\n",
      "base_model.model.base_model.model.classifier.modules_to_save.default.out_proj.weight\n",
      "base_model.model.base_model.model.classifier.modules_to_save.default.out_proj.bias\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainable parameters:\")\n",
    "count = 0\n",
    "for name, param in peft_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        count=count+1\n",
    "        print(name)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da45f85c-b016-4c49-8808-6eafa7cb5d1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T03:21:07.951152Z",
     "iopub.status.busy": "2025-04-12T03:21:07.950549Z",
     "iopub.status.idle": "2025-04-12T03:21:07.956592Z",
     "shell.execute_reply": "2025-04-12T03:21:07.955754Z",
     "shell.execute_reply.started": "2025-04-12T03:21:07.951131Z"
    },
    "id": "da45f85c-b016-4c49-8808-6eafa7cb5d1b",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT Model\n",
      "trainable params: 888,580 || all params: 125,537,288 || trainable%: 0.7078\n"
     ]
    }
   ],
   "source": [
    "print('PEFT Model')\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12284b58",
   "metadata": {
    "id": "12284b58"
   },
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ee64c43-fe38-479a-b3c5-7d939a3db4c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T03:21:22.007221Z",
     "iopub.status.busy": "2025-04-12T03:21:22.006652Z",
     "iopub.status.idle": "2025-04-12T03:21:22.011475Z",
     "shell.execute_reply": "2025-04-12T03:21:22.010573Z",
     "shell.execute_reply.started": "2025-04-12T03:21:22.007196Z"
    },
    "id": "0ee64c43-fe38-479a-b3c5-7d939a3db4c1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# To track evaluation accuracy during training\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "768b4917-65de-4e55-ae7f-698e287535d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T03:21:37.828474Z",
     "iopub.status.busy": "2025-04-12T03:21:37.827968Z",
     "iopub.status.idle": "2025-04-12T03:21:37.863981Z",
     "shell.execute_reply": "2025-04-12T03:21:37.862929Z",
     "shell.execute_reply.started": "2025-04-12T03:21:37.828451Z"
    },
    "id": "768b4917-65de-4e55-ae7f-698e287535d4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./roberta-lora-agnews-v2\",\n",
    "    per_device_train_batch_size=8,          # smaller batch\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,                    \n",
    "    learning_rate=1e-5,                     # lower LR \n",
    "    weight_decay=0.01,                      # regularization to reduce overfitting\n",
    "    warmup_ratio=0.1,                       # added warmup\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b848278",
   "metadata": {
    "id": "9b848278"
   },
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98d9d57d-b57f-4acc-80fb-fc5443e75515",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T03:21:48.386378Z",
     "iopub.status.busy": "2025-04-12T03:21:48.385601Z",
     "iopub.status.idle": "2025-04-12T03:21:48.390428Z",
     "shell.execute_reply": "2025-04-12T03:21:48.389725Z",
     "shell.execute_reply.started": "2025-04-12T03:21:48.386353Z"
    },
    "id": "98d9d57d-b57f-4acc-80fb-fc5443e75515",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # Remove the unused argument before forwarding\n",
    "        if \"num_items_in_batch\" in inputs:\n",
    "            inputs.pop(\"num_items_in_batch\")\n",
    "        return super().compute_loss(model, inputs, return_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69ddb8d5-5d35-4fc2-93f6-6ce39d4beeef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T03:21:58.986130Z",
     "iopub.status.busy": "2025-04-12T03:21:58.985456Z",
     "iopub.status.idle": "2025-04-12T03:21:59.360929Z",
     "shell.execute_reply": "2025-04-12T03:21:59.360353Z",
     "shell.execute_reply.started": "2025-04-12T03:21:58.986110Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,  \n",
    "    compute_metrics=compute_metrics,  \n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "544a41e9-d9cf-4824-8093-4ce84131ba9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T03:22:06.521612Z",
     "iopub.status.busy": "2025-04-12T03:22:06.521010Z",
     "iopub.status.idle": "2025-04-12T06:11:08.017796Z",
     "shell.execute_reply": "2025-04-12T06:11:08.017101Z",
     "shell.execute_reply.started": "2025-04-12T03:22:06.521592Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20625' max='20625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20625/20625 2:48:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.340800</td>\n",
       "      <td>0.279026</td>\n",
       "      <td>0.908100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.279000</td>\n",
       "      <td>0.263729</td>\n",
       "      <td>0.912100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.245800</td>\n",
       "      <td>0.261859</td>\n",
       "      <td>0.913900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20625, training_loss=0.34730484935876094, metrics={'train_runtime': 10141.0244, 'train_samples_per_second': 32.541, 'train_steps_per_second': 2.034, 'total_flos': 5.635662032118067e+16, 'train_loss': 0.34730484935876094, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5183be7e-514f-4e64-a6f4-314a827e6be5",
   "metadata": {
    "id": "5183be7e-514f-4e64-a6f4-314a827e6be5"
   },
   "source": [
    "## Evaluate Finetuned Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038198cf-0953-47e7-bd47-b073d05f8378",
   "metadata": {
    "id": "038198cf-0953-47e7-bd47-b073d05f8378"
   },
   "source": [
    "### Performing Inference on Custom Input\n",
    "Uncomment following functions for running inference on custom inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f88ad420-3f46-4eff-9d71-0ce388163062",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:11:31.708634Z",
     "iopub.status.busy": "2025-04-12T06:11:31.708346Z",
     "iopub.status.idle": "2025-04-12T06:11:31.713714Z",
     "shell.execute_reply": "2025-04-12T06:11:31.712868Z",
     "shell.execute_reply.started": "2025-04-12T06:11:31.708614Z"
    },
    "id": "f88ad420-3f46-4eff-9d71-0ce388163062",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def classify(model, tokenizer, text):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "    output = model(**inputs)\n",
    "\n",
    "    prediction = output.logits.argmax(dim=-1).item()\n",
    "\n",
    "    print(f'\\n Class: {prediction}, Label: {id2label[prediction]}, Text: {text}')\n",
    "    return id2label[prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc52bb94-5e13-4943-9225-a6d7fd053579",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:11:38.331698Z",
     "iopub.status.busy": "2025-04-12T06:11:38.331390Z",
     "iopub.status.idle": "2025-04-12T06:11:38.407113Z",
     "shell.execute_reply": "2025-04-12T06:11:38.406340Z",
     "shell.execute_reply.started": "2025-04-12T06:11:38.331678Z"
    },
    "id": "fc52bb94-5e13-4943-9225-a6d7fd053579",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Class: 1, Label: Sports, Text: Kederis proclaims innocence Olympic champion Kostas Kederis today left hospital ahead of his date with IOC inquisitors claiming his ...\n",
      "\n",
      " Class: 2, Label: Business, Text: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindlinand of ultra-cynics, are seeing green again.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Business'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify( peft_model, tokenizer, \"Kederis proclaims innocence Olympic champion Kostas Kederis today left hospital ahead of his date with IOC inquisitors claiming his ...\")\n",
    "classify( peft_model, tokenizer, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a3e276-bf8c-4403-8a48-5ef19f2beccf",
   "metadata": {
    "id": "68a3e276-bf8c-4403-8a48-5ef19f2beccf"
   },
   "source": [
    "### Run Inference on eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3685c320-98ed-40d2-b43a-1898c298de73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:11:55.799768Z",
     "iopub.status.busy": "2025-04-12T06:11:55.799474Z",
     "iopub.status.idle": "2025-04-12T06:11:59.297965Z",
     "shell.execute_reply": "2025-04-12T06:11:59.297026Z",
     "shell.execute_reply.started": "2025-04-12T06:11:55.799747Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.16)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebbc20a2-a1c0-4cb7-b842-f52e4de61ed5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:24:20.491010Z",
     "iopub.status.busy": "2025-04-12T06:24:20.490661Z",
     "iopub.status.idle": "2025-04-12T06:24:20.500414Z",
     "shell.execute_reply": "2025-04-12T06:24:20.499597Z",
     "shell.execute_reply.started": "2025-04-12T06:24:20.490989Z"
    },
    "id": "ebbc20a2-a1c0-4cb7-b842-f52e4de61ed5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def evaluate_model(inference_model, dataset, labelled=True, batch_size=8, data_collator=None):\n",
    "    \"\"\"\n",
    "    Evaluate a PEFT model on a dataset.\n",
    "\n",
    "    Returns:\n",
    "        If labelled is True:\n",
    "            - metrics (dict)\n",
    "            - predictions (tensor)\n",
    "            - true labels (tensor)\n",
    "        Else:\n",
    "            - predictions only\n",
    "    \"\"\"\n",
    "    eval_dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=data_collator)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    inference_model.to(device)\n",
    "    inference_model.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    if labelled:\n",
    "        accuracy_metric = evaluate.load(\"accuracy\")\n",
    "        precision_metric = evaluate.load(\"precision\")\n",
    "        recall_metric = evaluate.load(\"recall\")\n",
    "        f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = inference_model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "        all_predictions.append(predictions.cpu())\n",
    "\n",
    "        if labelled:\n",
    "            labels = batch[\"labels\"].cpu()\n",
    "            all_labels.append(labels)\n",
    "\n",
    "            accuracy_metric.add_batch(predictions=predictions.cpu().numpy(), references=labels.numpy())\n",
    "            precision_metric.add_batch(predictions=predictions.cpu().numpy(), references=labels.numpy())\n",
    "            recall_metric.add_batch(predictions=predictions.cpu().numpy(), references=labels.numpy())\n",
    "            f1_metric.add_batch(predictions=predictions.cpu().numpy(), references=labels.numpy())\n",
    "\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "\n",
    "    if labelled:\n",
    "        all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "        eval_metric = {\n",
    "            \"accuracy\": accuracy_metric.compute(),\n",
    "            \"precision\": precision_metric.compute(average=\"macro\"),\n",
    "            \"recall\": recall_metric.compute(average=\"macro\"),\n",
    "            \"f1\": f1_metric.compute(average=\"macro\"),\n",
    "        }\n",
    "\n",
    "        print(\"Evaluation Metrics:\", eval_metric)\n",
    "        return eval_metric, all_predictions, all_labels\n",
    "    else:\n",
    "        return all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "809635a6-a2c7-4d09-8d60-ababd1815003",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:24:24.961059Z",
     "iopub.status.busy": "2025-04-12T06:24:24.960783Z",
     "iopub.status.idle": "2025-04-12T06:27:33.092839Z",
     "shell.execute_reply": "2025-04-12T06:27:33.091981Z",
     "shell.execute_reply.started": "2025-04-12T06:24:24.961041Z"
    },
    "id": "809635a6-a2c7-4d09-8d60-ababd1815003",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [03:06<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics: {'accuracy': {'accuracy': 0.9139}, 'precision': {'precision': 0.9137724000386551}, 'recall': {'recall': 0.9133416429141182}, 'f1': {'f1': 0.9132504496414808}}\n"
     ]
    }
   ],
   "source": [
    "metrics, predictions, labels = evaluate_model(\n",
    "    model,\n",
    "    eval_dataset,\n",
    "    labelled=True,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "kMJgvV1ZnVhd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:29:25.597494Z",
     "iopub.status.busy": "2025-04-12T06:29:25.596700Z",
     "iopub.status.idle": "2025-04-12T06:29:25.614948Z",
     "shell.execute_reply": "2025-04-12T06:29:25.614254Z",
     "shell.execute_reply.started": "2025-04-12T06:29:25.597471Z"
    },
    "id": "kMJgvV1ZnVhd",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-Class Metrics:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.94      0.89      0.91      2530\n",
      "      Sports       0.96      0.99      0.97      2528\n",
      "    Business       0.88      0.87      0.87      2407\n",
      "    Sci/Tech       0.88      0.91      0.89      2535\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.91      0.91      0.91     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class_names = dataset.features[\"label\"].names\n",
    "\n",
    "print(\"\\nPer-Class Metrics:\\n\")\n",
    "print(classification_report(labels.numpy(), predictions.numpy(), target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ac0e39d-1d0b-4364-9c83-a9a1c6da9bb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:29:44.517174Z",
     "iopub.status.busy": "2025-04-12T06:29:44.516896Z",
     "iopub.status.idle": "2025-04-12T06:29:44.522856Z",
     "shell.execute_reply": "2025-04-12T06:29:44.522064Z",
     "shell.execute_reply.started": "2025-04-12T06:29:44.517155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_per_class_f1(preds, labels, class_names):\n",
    "    report = classification_report(\n",
    "        labels.numpy(),\n",
    "        preds.numpy(),\n",
    "        target_names=class_names,\n",
    "        output_dict=True\n",
    "    )\n",
    "    f1_scores = {cls: report[cls][\"f1-score\"] for cls in class_names}\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(f1_scores.keys(), f1_scores.values())\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.title(\"Per-Class F1 Scores\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d911b87c-c417-4d2f-a5ed-1c4d1febad2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:29:51.911744Z",
     "iopub.status.busy": "2025-04-12T06:29:51.911086Z",
     "iopub.status.idle": "2025-04-12T06:29:52.206166Z",
     "shell.execute_reply": "2025-04-12T06:29:52.205432Z",
     "shell.execute_reply.started": "2025-04-12T06:29:51.911709Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_per_class_f1(predictions, labels, class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f39087-f2bb-49d3-9fe1-0d812fb30203",
   "metadata": {
    "id": "75f39087-f2bb-49d3-9fe1-0d812fb30203"
   },
   "source": [
    "### Run Inference on unlabelled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2af62541-2c33-4f16-bb1c-cc969c715cd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:30:03.024533Z",
     "iopub.status.busy": "2025-04-12T06:30:03.023785Z",
     "iopub.status.idle": "2025-04-12T06:30:09.224276Z",
     "shell.execute_reply": "2025-04-12T06:30:09.223551Z",
     "shell.execute_reply.started": "2025-04-12T06:30:03.024506Z"
    },
    "id": "2af62541-2c33-4f16-bb1c-cc969c715cd7",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980d47aab60d491e8454563b91d0af01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 8000\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load your unlabelled data\n",
    "unlabelled_dataset = pd.read_pickle(\"/kaggle/input/deep-learning-spring-2025-project-2/test_unlabelled.pkl\")\n",
    "test_dataset = unlabelled_dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "unlabelled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e60991d3-38b1-4657-8854-408ce66f6b84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:36:01.317597Z",
     "iopub.status.busy": "2025-04-12T06:36:01.316825Z",
     "iopub.status.idle": "2025-04-12T06:37:51.200485Z",
     "shell.execute_reply": "2025-04-12T06:37:51.199720Z",
     "shell.execute_reply.started": "2025-04-12T06:36:01.317571Z"
    },
    "id": "e60991d3-38b1-4657-8854-408ce66f6b84",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:49<00:00,  9.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete. Predictions saved to inference_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run inference and save predictions\n",
    "preds = evaluate_model(peft_model, test_dataset, False, 8, data_collator)\n",
    "df_output = pd.DataFrame({\n",
    "    'ID': range(len(preds)),\n",
    "    'Label': preds.numpy()  # or preds.tolist()\n",
    "})\n",
    "df_output.to_csv(os.path.join(\"inference_output.csv\"), index=False)\n",
    "print(\"Inference complete. Predictions saved to inference_output.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
